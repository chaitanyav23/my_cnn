{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0526b2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "\n",
    "        # Weight matrix: (out_features, in_features)\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.randn(out_features, in_features) * 0.01\n",
    "        )\n",
    "\n",
    "        # Bias vector: (out_features,)\n",
    "        self.bias = nn.Parameter(\n",
    "            torch.zeros(out_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape: (batch_size, in_features)\n",
    "        output shape: (batch_size, out_features)\n",
    "        \"\"\"\n",
    "        return x @ self.weight.T + self.bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2ba08e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4, 10)\n",
    "fc = CustomLinear(10, 3)\n",
    "y = fc(x)\n",
    "\n",
    "print(y.shape)\n",
    "# torch.Size([4, 3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9cf51a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomReLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.clamp(x, min=0.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e5bebac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[-1.0, 2.0, -0.5]])\n",
    "relu = CustomReLU()\n",
    "print(relu(x))\n",
    "# tensor([[0., 2., 0.]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fd38ce9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomMaxPool2D(nn.Module):\n",
    "    def __init__(self, kernel_size, stride=None):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride if stride is not None else kernel_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape: (B, C, H, W)\n",
    "        \"\"\"\n",
    "        B, C, H, W = x.shape\n",
    "        k = self.kernel_size\n",
    "        s = self.stride\n",
    "\n",
    "        # Unfold: (B, C*k*k, L)\n",
    "        x_unfold = F.unfold(\n",
    "            x,\n",
    "            kernel_size=k,\n",
    "            stride=s\n",
    "        )\n",
    "\n",
    "        # Reshape to separate window elements\n",
    "        # (B, C, k*k, L)\n",
    "        x_unfold = x_unfold.view(B, C, k * k, -1)\n",
    "\n",
    "        # Max over window\n",
    "        out, _ = torch.max(x_unfold, dim=2)\n",
    "\n",
    "        # Compute output spatial size\n",
    "        H_out = (H - k) // s + 1\n",
    "        W_out = (W - k) // s + 1\n",
    "\n",
    "        # Reshape back to image format\n",
    "        out = out.view(B, C, H_out, W_out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "490f3bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 6.,  8.],\n",
      "          [14., 16.]]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[[[1., 2., 3., 4.],\n",
    "                    [5., 6., 7., 8.],\n",
    "                    [9.,10.,11.,12.],\n",
    "                    [13.,14.,15.,16.]]]])\n",
    "\n",
    "pool = CustomMaxPool2D(kernel_size=2)\n",
    "y = pool(x)\n",
    "\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c7279e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomConv2D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        # Weight: (out_channels, in_channels, k, k)\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.randn(out_channels, in_channels, kernel_size, kernel_size) * 0.01\n",
    "        )\n",
    "\n",
    "        # Bias: (out_channels,)\n",
    "        self.bias = nn.Parameter(torch.zeros(out_channels))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape: (B, C_in, H, W)\n",
    "        \"\"\"\n",
    "        B, C, H, W = x.shape\n",
    "        k = self.kernel_size\n",
    "\n",
    "        # Step 1: Unfold input\n",
    "        # Shape: (B, C_in * k * k, L)\n",
    "        x_unfold = F.unfold(\n",
    "            x,\n",
    "            kernel_size=k,\n",
    "            stride=self.stride,\n",
    "            padding=self.padding\n",
    "        )\n",
    "\n",
    "        # Step 2: Reshape weights\n",
    "        # (out_channels, C_in * k * k)\n",
    "        w_flat = self.weight.view(self.out_channels, -1)\n",
    "\n",
    "        # Step 3: Perform convolution as matrix multiplication\n",
    "        # (B, out_channels, L)\n",
    "        out = w_flat @ x_unfold\n",
    "        out = out + self.bias.view(1, -1, 1)\n",
    "\n",
    "        # Step 4: Reshape output to image grid\n",
    "        H_out = (H + 2 * self.padding - k) // self.stride + 1\n",
    "        W_out = (W + 2 * self.padding - k) // self.stride + 1\n",
    "\n",
    "        out = out.view(B, self.out_channels, H_out, W_out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9e193508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 3, 8, 8)\n",
    "\n",
    "conv_ref = nn.Conv2d(3, 5, 3, padding=1, bias=True)\n",
    "conv_custom = CustomConv2D(3, 5, 3, padding=1)\n",
    "\n",
    "# Copy weights for comparison\n",
    "conv_custom.weight.data = conv_ref.weight.data.clone()\n",
    "conv_custom.bias.data = conv_ref.bias.data.clone()\n",
    "\n",
    "y_ref = conv_ref(x)\n",
    "y_custom = conv_custom(x)\n",
    "\n",
    "print(torch.allclose(y_ref, y_custom, atol=1e-6))\n",
    "# True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e3337695",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = CustomConv2D(\n",
    "            in_channels=1,\n",
    "            out_channels=8,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        )\n",
    "        self.relu1 = CustomReLU()\n",
    "        self.pool1 = CustomMaxPool2D(kernel_size=2)\n",
    "\n",
    "        self.conv2 = CustomConv2D(\n",
    "            in_channels=8,\n",
    "            out_channels=12,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        )\n",
    "        self.relu2 = CustomReLU()\n",
    "        self.pool2 = CustomMaxPool2D(kernel_size=2)\n",
    "\n",
    "        self.conv3 = CustomConv2D(\n",
    "            in_channels=12,\n",
    "            out_channels=16,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        )\n",
    "        self.relu3 = CustomReLU()\n",
    "        self.pool3 = CustomMaxPool2D(kernel_size=2)        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, 1, H, W)\n",
    "        returns: (B, D)\n",
    "        \"\"\"\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = self.pool3(self.relu3(self.conv3(x)))\n",
    "\n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8393e12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiInputCNN(nn.Module):\n",
    "    def __init__(self, input_shape, num_classes=16):\n",
    "        \"\"\"\n",
    "        input_shape: (1, H, W) â€” shape of ONE input image\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.feature_extractor = FeatureExtractor()\n",
    "\n",
    "        # ---- Automatically compute feature dimension ----\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, *input_shape)\n",
    "            dummy_feat = self.feature_extractor(dummy)\n",
    "            feature_dim = dummy_feat.shape[1]\n",
    "\n",
    "        self.feature_dim = feature_dim\n",
    "\n",
    "        # ---- Classifier ----\n",
    "        self.fc1 = CustomLinear(3 * feature_dim, 32)\n",
    "        self.relu = CustomReLU()\n",
    "        self.fc2 = CustomLinear(32, num_classes)\n",
    "\n",
    "    def forward(self, x1, x2, x3):\n",
    "        \"\"\"\n",
    "        x1, x2, x3: (B, 1, H, W)\n",
    "        \"\"\"\n",
    "        f1 = self.feature_extractor(x1)\n",
    "        f2 = self.feature_extractor(x2)\n",
    "        f3 = self.feature_extractor(x3)\n",
    "\n",
    "        fused = torch.cat([f1, f2, f3], dim=1)\n",
    "        out = self.fc2(self.relu(self.fc1(fused)))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2a3763c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "class MultiInputImageDataset(Dataset):\n",
    "    def __init__(self, csv_path, root_dir=None, transform=None):\n",
    "        \"\"\"\n",
    "        csv_path : path to metadata CSV\n",
    "        root_dir : optional base directory for images\n",
    "        transform: torchvision-style transform (applied to ALL 3 images)\n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _load_image(self, path):\n",
    "        if self.root_dir is not None:\n",
    "            path = os.path.join(self.root_dir, path)\n",
    "\n",
    "        img = Image.open(path).convert(\"L\")  # grayscale\n",
    "        img = img.resize((128, 128))\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        else:\n",
    "            # fallback: tensor + normalize to [0,1]\n",
    "            img = torch.from_numpy(\n",
    "                np.array(img, dtype=\"float32\") / 255.0\n",
    "            ).unsqueeze(0)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        img1_path = row[\"input_1\"]\n",
    "        img2_path = row[\"input_2\"]\n",
    "        img3_path = row[\"input_3\"]\n",
    "        label = int(row[\"target\"])\n",
    "\n",
    "        img1 = self._load_image(img1_path)\n",
    "        img2 = self._load_image(img2_path)\n",
    "        img3 = self._load_image(img3_path)\n",
    "\n",
    "        return img1, img2, img3, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "72acdf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "df = pd.read_csv(\"dataset_music/train/metadata.csv\")\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df[\"target\"]\n",
    ")\n",
    "\n",
    "train_df.to_csv(\"dataset_music/train/train_split.csv\", index=False)\n",
    "val_df.to_csv(\"dataset_music/train/val_split.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0759fa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MultiInputImageDataset(\n",
    "    csv_path=\"dataset_music/train/train_split.csv\",\n",
    "    root_dir=None,\n",
    ")\n",
    "\n",
    "val_dataset = MultiInputImageDataset(\n",
    "    csv_path=\"dataset_music/train/val_split.csv\",\n",
    "    root_dir=None,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f054fbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "24eb2df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def evaluate_macro_f1(model, loader, device):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img1, img2, img3, target in loader:\n",
    "            img1 = img1.to(device)\n",
    "            img2 = img2.to(device)\n",
    "            img3 = img3.to(device)\n",
    "\n",
    "            logits = model(img1, img2, img3)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "            y_true.extend(target.numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    return f1_score(y_true, y_pred, average=\"macro\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3ab1b529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 128]) torch.Size([1, 128, 128]) torch.Size([1, 128, 128]) 8\n"
     ]
    }
   ],
   "source": [
    "img1, img2, img3, label = train_dataset[0]\n",
    "print(img1.shape, img2.shape, img3.shape, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7812af6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396476\n"
     ]
    }
   ],
   "source": [
    "model = MultiInputCNN(input_shape=(1, 128, 128))\n",
    "print(sum(p.numel() for p in model.parameters()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1ced2dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] Loss: 2.6590 | Val Macro F1: 0.0777\n",
      "Epoch [2/10] Loss: 1.9775 | Val Macro F1: 0.3329\n",
      "Epoch [3/10] Loss: 1.7066 | Val Macro F1: 0.3600\n",
      "Epoch [4/10] Loss: 1.5821 | Val Macro F1: 0.4109\n",
      "Epoch [5/10] Loss: 1.5092 | Val Macro F1: 0.4195\n",
      "Epoch [6/10] Loss: 1.4581 | Val Macro F1: 0.4531\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img1, img2, img3, target \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m---> 14\u001b[0m     img1 \u001b[38;5;241m=\u001b[39m \u001b[43mimg1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     img2 \u001b[38;5;241m=\u001b[39m img2\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     16\u001b[0m     img3 \u001b[38;5;241m=\u001b[39m img3\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for img1, img2, img3, target in train_loader:\n",
    "        img1 = img1.to(device)\n",
    "        img2 = img2.to(device)\n",
    "        img3 = img3.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        logits = model(img1, img2, img3)\n",
    "        loss = criterion(logits, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    val_f1 = evaluate_macro_f1(model, val_loader, device)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "        f\"Loss: {avg_loss:.4f} | Val Macro F1: {val_f1:.4f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55291d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs776",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
